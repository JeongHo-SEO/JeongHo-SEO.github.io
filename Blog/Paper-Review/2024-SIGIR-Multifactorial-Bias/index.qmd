---
title: "[SIGIR 2024] Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems"
date: "2026-02-24"
description: "Summary of this paper briefly"
toc: true
number-sections: true
---
- References
    - **Paper**: [Paper Title](https://dl.acm.org/doi/10.1145/3626772.3657749)
    - **Official Code**: [GitHub Repository](https://github.com/BetsyHJ/MultifactorialBias)
    - **Conference**: [SIGIR 2024](https://dl.acm.org/doi/proceedings/10.1145/3626772)

# Overview & Problem (Motivation)

## Selection Bias
ìš°ë¦¬ëŠ” ì‹¤ì œ ëª¨ì§‘ë‹¨(population)ì˜ distributionì„ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, í‘œë³¸(sample)ì„ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•œë‹¤.  
ì´ ë•Œ í‘œë³¸ì´ ëª¨ì§‘ë‹¨ì˜ distributionì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°, **Sample**ì— **Selection Bias**ê°€ ì¡´ìž¬í•œë‹¤ê³  í•  ìˆ˜ ìžˆë‹¤. [Ovaisi et al. definition]  
- Sampling Biasë¼ê³  ì¼ì»¬ì–´ë„ ì¢‹ì„ ê²ƒ ê°™ì€ë°, "Sampling" ì£¼ì²´ê°€ Algorithmì´ë‹ˆ Selection Biasë¼ê³  í•˜ëŠ” ê²ƒ ê°™ë‹¤.  

ì´ëŸ¬í•œ Selection BiasëŠ” userê°€ íŠ¹ì • itemê³¼ interactionì„ ë§Žì´ í•˜ëŠ” *self-selection bias*ì™€, ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì•Œê³ ë¦¬ì¦˜ì´ userì—ê²Œ itemì„ ì¶”ì²œí•˜ëŠ” *algorithmic bias*ì— ì˜í•´ ë°œìƒí•œë‹¤.  

ë˜í•œ, ì„ íƒ íŽ¸í–¥ì€ í¬ê²Œ ***Popularity Bias***ì™€ ***Positivity Bias***ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìžˆë‹¤.  

### Popularity Bias
ì¸ê¸° íŽ¸í–¥ì˜ ê²½ìš° userê°€ popular itemì— ë” ë§Žì€ í”¼ë“œë°±ì„ ì œê³µí•˜ê±°ë‚˜, popular itemì´ ì§€ë‹Œ popularityì— ë¹„í•´ ë” ë§Žì´ ì¶”ì²œë˜ëŠ” ê²½ìš° ë°œìƒí•œë‹¤.  
ë”°ë¼ì„œ ê° itemì— ëŒ€í•˜ì—¬ interactionì„ ì¹´ìš´íŠ¸í•˜ë©´, íŠ¹ì • ì¸ê¸° ìžˆëŠ” itemì—ë§Œ interactionì´ ì ë¦¬ëŠ” ***long-tail distribution**ì´ ê´€ì°°ëœë‹¤.

### Positivity Bias
ê¸ì • íŽ¸í–¥ì˜ ê²½ìš° userê°€ ì¢‹ì•„í•˜ê±°ë‚˜(5ì )/ì‹«ì–´í•˜ëŠ”(1ì ) itemì— ë” ë§Žì€ í‰ê°€(rate)ë¥¼ í•˜ëŠ” ê²½ìš° ë°œìƒí•œë‹¤. ì¦‰, ê·¸ì € ê·¸ëž¬ë˜ (3ì ) itemì˜ ê²½ìš° í‰ê°€ë¥¼ ë°›ì§€ ëª»í•˜ëŠ” ê²½í–¥ì´ í¬ë‹¤.  
ë”°ë¼ì„œ ëª¨ì§‘ë‹¨ì˜ rating distributionê³¼ ë¹„êµí–ˆì„ ë•Œ, í‘œë³¸ì˜ rating distributionì€ ì–‘ê·¹ë‹¨ (í˜¹ì€ í•œìª½ ê·¹ë‹¨) itemì— userì˜ í‰ê°€ê°€ ì ë¦°, ***skewed distribution***ì´ ê´€ì°°ëœë‹¤.  
ì •ë¦¬í•˜ìžë©´, userì˜ preference $y$ì— ì˜í•´ rating $r$ì˜ ë¶„í¬ê°€ skewedëœë‹¤.


## Problem

### Single-factor bias
ê¸°ì¡´ì— ì¡´ìž¬í•˜ëŠ” ì—°êµ¬ì˜ ê²½ìš°, popularity bias í˜¹ì€ positivity biasë§Œì„ ê³ ë ¤í•˜ì—¬ selection biasë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ë²•ë“¤ì´ ì‚¬ìš©ë˜ì—ˆë‹¤.  

### IDEA: Multi-factorial Bias
real-worldì—ì„œëŠ” ì—¬ëŸ¬ biasê°€ ë™ì‹œì— ë°œê²¬ë˜ê¸° ë•Œë¬¸ì—, ì•žì—ì„œ ì–¸ê¸‰í•œ ì£¼ìš” ë‘ ê°€ì§€ bias (popularity bias, positivity bias)ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ selection biasë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ë²•ë¡ ì´ ë³¸ ë…¼ë¬¸ì—ì„œ ì œê¸°ë˜ì—ˆë‹¤.  
ë˜í•œ multi-factorial biasë¥¼ ì¶”ì •í•  ë•Œ, ê¸°ì¡´ Single-factor bias ëª¨í˜•(baselines)ì˜ SOTAë³´ë‹¤ íš¨ê³¼ì (effective)ì´ë©° ê°•ê±´(robust)í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•˜ì˜€ìŒì„ ë³¸ ë…¼ë¬¸ì˜ ì €ìžëŠ” ë°ížŒë‹¤.


# Preliminaries

## Notation
ì´ëŸ¬í•œ Notation ë° Definitionë“¤ì€ Publicationsë§ˆë‹¤ ì¡°ê¸ˆì”© ë‹¬ë¼ì„œ, í•œ ë²ˆ ì •ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.  
íŠ¹ížˆ, ë…¼ë¬¸ì—ëŠ” $y$ë¥¼ ratingì´ë¼ê³  í‘œí˜„í•˜ì§€ë§Œ, ì •í™•ížˆëŠ” preferenceë¡œ ë³´ëŠ” ê²ƒì´ ì ì ˆí•´ë³´ì¸ë‹¤. (ë¬¸ë§¥ìƒ)  

- user set: $\mathcal{U} = \{u_1, \ldots, u_N\}$  
- item set: $\mathcal{I} = \{i_1, \ldots, i_M\}$  

- rating score set: $\mathcal{R} = \{1, 2, 3, 4, 5\}$  
  - preference score variable: $y_{u,i} \in \mathcal{R}$  
    - í˜¼ë™ì„ í”¼í•˜ê¸° ìœ„í•´ ì–¸ê¸‰í•˜ìžë©´, DBì— ê¸°ë¡ëœ ê°’ì´ ì•„ë‹Œ, user uê°€ item ië¥¼ ì†Œë¹„í–ˆë‹¤ë©´ ë§¤ê²¼ì„ True Preferenceì´ë‹¤. $\forall (u,i)$ì—ì„œ ì¡´ìž¬í•˜ëŠ” ê°’.  
  - rating score variable: $r_{u,i} \in \mathcal{R}$: ì‹¤ì œ DBì— ê¸°ë¡ëœ ê°’, user uê°€ item iì— ëŒ€í•´ ì‹¤ì œë¡œ ratingí•œ ê°’. $(o_{u,i}=1)$ì¼ ë•Œ.  
    - $r_{u,i} = o_{u,i} * y_{u,i}$ë¡œ ëª¨ë¸ë§ ê°€ëŠ¥í•  ë“¯ ì‹¶ë‹¤..  

- observition set: $\{0, 1\}$ - rated or not  
  - observation  
  - observation variable: $o_{u,i} \in \{0, 1\}$  
  - observation matrix: $\mathcal{O} = \{o_{u,i}\}$ - sparse by Positivity Bias  

- logged rating (sample) dataset: $\mathcal{D} = \{\left(u, i, y_{u,i}\right) | o_{u,i} = 1 \}$  
> ê´€ì°°ëœ (user, item, preference) tuple. $\leftarrow$ rating tuple ($r_{u,i}$)ë¡œ ë´ë„ ë¬´ë°©. $\forall (u,i) \in \mathcal{D}: y_{u,i}=r_{u,i}$.  

## Definition

### Propensity Score
$p_{u,i} = P\left(o_{u,i}=1 | u,i,y_{u,i}\right)$: ì£¼ì–´ì§„ (u,i) pairì—ì„œ userê°€ itemì„ observing í–ˆì„ í™•ë¥ ì´ë‹¤.   
ë‹¤ë¥¸ ë…¼ë¬¸([PROPCARE](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a237f11d6aad94f59a182d70405d3fdb-Abstract-Conference.html))ì—ì„œëŠ” Exposure Probabilityë¡œ ì •ì˜í•˜ì˜€ëŠ”ë°, Exposure == Observation ë¡œ ë³´ë©´ ì¼ë§¥ìƒí†µ í•œë‹¤. (Publications ë§¥ë½ë§ˆë‹¤ ë‹¤ë¥¸ ê²ƒ ê°™ë‹¤.)  
í†µê³„í•™ì—ì„œëŠ” íŠ¹ì • condition Xê°€ givenì¼ ë•Œ, Treatment Të¥¼ ë°›ì„ í™•ë¥ ë¡œ Propensity Scoreë¥¼ ì •ì˜í•œë‹¤.  
- $e(X)=P(T=1|X)$

### Selection Bias
=> Selection Biasê°€ ì—†ë‹¤ë©´ ëª¨ë“  í‘œë³¸ì˜ ë¶„í¬ë“¤ê³¼ ëª¨ì§‘ë‹¨ì˜ ë¶„í¬ê°€ ê°™ë‹¤. (identical distribution)  
=> ë”°ë¼ì„œ ìž„ì˜ì˜ (user, item) pairë¥¼ ì„ ì •í–ˆì„ ë•Œ, observing prob.ì¸ $p_{u,i}$ ë˜í•œ ê°™ì„ ê²ƒì´ë‹¤. (missing completely at random - MCAR)  
=> ë°˜ëŒ€ë¡œ, $p_{u,i}$ê°€ (user, item) pairì— ì˜í•´ ê°’ì´ ë³€í•œë‹¤ë©´, identical distributionì´ ì•„ë‹ˆë¯€ë¡œ Selection Biasê°€ ì¡´ìž¬í•œë‹¤.

$$ Selection-bias\left(\mathcal{D}\right) \Leftrightarrow \exists u,u',i,i': p_{u,i} \neq p_{u', i'}$$


### Positivity and Popularity Bias
fixed user uì— ëŒ€í•˜ì—¬, itemì— ë”°ë¼ preference scoreê°€ ê²°ì •ëœë‹¤. $(I \rightarrow Y)$  

**(Fig. 1a)** Positivity biasì—ì„œëŠ” preference scoreê°€ ë†’ì„ ìˆ˜ë¡ observation í™•ë¥  ë˜í•œ ë†’ì•„ì§„ë‹¤. $(Y \rightarrow O)$  
ë”°ë¼ì„œ rating scoreì™€ propensityê°€ ë¹„ë¡€í•´ì•¼ í•˜ë¯€ë¡œ,
$$ Positivity-bias\left(\mathcal{D}\right) \Leftrightarrow Selection-bias\left(\mathcal{D}\right) \land \left(y_{u,i} > y_{u', i'} \leftrightarrow p_{u,i} > p_{u', i'}\right)$$
ê´€ê³„ê°€ ì„±ë¦½í•œë‹¤.  


**(Fig. 1b)** í•œíŽ¸, popularity biasì˜ ê²½ìš°, userì™€ ë¬´ê´€í•˜ê²Œ itemì´ ì¸ê¸°ìžˆì„ ìˆ˜ë¡ observation í™•ë¥ ì´ ì»¤ì§„ë‹¤. $(I \rightarrow O)$  
ë”°ë¼ì„œ ë™ì¼í•œ itemì— ëŒ€í•˜ì—¬ propensity scoreê°€ ê°™ì•„ì•¼ í•˜ë¯€ë¡œ,  
$$ Popularity-bias\left(\mathcal{D}\right) \Leftrightarrow Selection-bias\left(\mathcal{D}\right) \land \left( i=i' \rightarrow p_{u,i} = p_{u', i'}\right)$$
ê´€ê³„ê°€ ì„±ë¦½í•œë‹¤.  


![figure1](figures/figure-1.png)

### Multi-factorial Bias
**(Fig. 1c)** ê²°ê³¼ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì£¼ìš” biasë¥¼ ëª¨ë‘ ê³ ë ¤í•œ, Multi-factorial Bias ëª¨í˜•ì„ ì‚¬ìš©í•œë‹¤.  
ë”°ë¼ì„œ $(Y \rightarrow O), (I \rightarrow O)$ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•œë‹¤.
$$ Multifactorial-bias\left(\mathcal{D}\right) \Leftrightarrow Selection-bias\left(\mathcal{D}\right) \land \left( i=i' \land y_{u,i} = y_{u', i'}\rightarrow p_{u,i} = p_{u', i'}\right)$$

## Loss functions & Estimation
- Loss functionì˜ ê²½ìš° observationsë“¤ì„ í†µí•´ ê³„ì‚°í•¨ì„ ìœ ë…í•˜ë©´, preference $y$ê°€ ì•„ë‹Œ rating $r$ë¡œ ê³„ì‚°ë¨ì„ ì´í•´í•  ìˆ˜ ìžˆë‹¤.

### Naive Loss
Actual preference scoreì¸ $y_{u,i}$ì™€ Predicted preference scoreì¸ $\hat{y}_{u,i}$ê°„ì˜ ì˜¤ì°¨ë¥¼ ì´ìš©í•˜ì—¬ loss functionì„ ë§Œë“¤ ìˆ˜ ìžˆë‹¤.  
MSEì™€ ê°™ì€ ì˜¤ì°¨ í•¨ìˆ˜ $\delta(\hat{y}_{u,i}, y_{u,i})$ë¥¼ ì‚¬ìš©í•˜ì—¬, (ì‚¬ì‹¤ í†µê³„ì ìœ¼ë¡œëŠ” ìž”ì°¨ í•¨ìˆ˜ì´ë‹¤. prediction - observation ì´ë¼ì„œ.)
$$\mathcal{L} = \frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u \in \mathcal{U}} \sum_{i \in \mathcal{I}} \delta(\hat{y}_{u,i}, y_{u,i}) $$
ì™€ ê°™ì€ ë°©ì‹ì´ë‹¤.

ê·¸ëŸ¬ë‚˜, ëª¨ë“  (user, item) pairì—ì„œ actual preference scoreì„ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ideal lossì¸ $\mathcal{L}$ì„ êµ¬í•  ìˆ˜ ì—†ë‹¤.  
ë”°ë¼ì„œ ìš°ë¦¬ëŠ” logged rating datasetì¸ $\mathcal{D}$ì—ì„œ rating scoreì¸ $r_{u,i}$ë¥¼ í™œìš©í•˜ì—¬, loss functionì„ ê³„ì‚°í•œë‹¤.  
$$\mathcal{L}_{\text{Naive}}
= \frac{1}{|\mathcal{D}|}\sum_{u,i \in \mathcal{D}} \delta(\hat{y}_{u,i}, y_{u,i})
=\frac{1}{|\mathcal{D}|}\sum_{u,i \in \mathcal{D}} \delta(\hat{r}_{u,i}, r_{u,i})
$$

í•œíŽ¸, rating datasetì¸ $\mathcal{D}$ë§ˆë‹¤ $\mathcal{L}_{\text{Naive}}$ ê°’ì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì—,  
ìš°ë¦¬ëŠ” ì´ê²ƒì˜ ê¸°ëŒ“ê°’ì¸ $\mathbb{E}\mathcal{L}_{\text{Naive}} = \frac{1}{|\mathcal{D}|}\sum_{u \in \mathcal{U}} \sum_{i \in \mathcal{I}} p_{u,i} \cdot \delta(\hat{y}_{u,i}, y_{u,i})$ ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ naive loss functionì€ ì•žì—ì„œ ë…¼ì˜í•œ selection biasë¥¼ ì „í˜€ ë°˜ì˜í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì—, ***IPS loss***ë¥¼ ì‚¬ìš©í•œë‹¤.

### IPS(Inverse Propensity Scoring) Loss
IPS methodëŠ” selection bias íš¨ê³¼ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìžì£¼ ì‚¬ìš©ëœë‹¤. propensity scoreê°€ ë†’ì€ (user, item) pairëŠ” ê·¸ë§Œí¼ rating í™•ë¥ ì´ í¬ê¸° ë•Œë¬¸ì—, ì˜¤ì°¨ í•¨ìˆ˜ì˜ ê°’ì„ ì¤„ì—¬ì£¼ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ì•¼ í•œë‹¤.  
ë”°ë¼ì„œ ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ë¥¼ propensity socreì˜ ì—­ìˆ˜ë¥¼ ì ìš©í•¨ìœ¼ë¡œì„œ, $\mathcal{L}_{\text{IPS}} = \frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u,i \in \mathcal{D}}\frac{1}{p_{u,i}}\delta(\hat{r}_{u,i}, r_{u,i})$ ë¡œ IPS lossë¥¼ ì •ì˜í•œë‹¤.

ì´ ë•Œ, ìœ„ì™€ ê°™ì€ ë…¼ë¦¬ë¡œ ê¸°ëŒ“ê°’ì„ ì ìš©í•˜ì—¬ ì´ëŸ¬í•œ IPS lossì˜ í‰ê· ì„ ë‚´ë©´,
$$
\begin{aligned}
\mathbb{E}\mathcal{L}_{\text{IPS}}
&= \mathbb{E}\frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u,i \in \mathcal{D}} \frac{1}{p_{u,i}}\delta(\hat{r}_{u,i}, r_{u,i}) \\
&= \mathbb{E}\frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u \in \mathcal{U}} \sum_{i \in \mathcal{I}} o_{u,i}\frac{1}{p_{u,i}}\delta(\hat{y}_{u,i}, y_{u,i}) \\
&= \frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u \in \mathcal{U}} \sum_{i \in \mathcal{I}}\frac{\mathbb{E} o_{u,i}}{p_{u,i}}\delta(\hat{y}_{u,i}, y_{u,i}) \\
&= \frac{1}{|\mathcal{U}||\mathcal{I}|}\sum_{u \in \mathcal{U}} \sum_{i \in \mathcal{I}}\delta(\hat{y}_{u,i}, y_{u,i}) \quad \left(\because \mathbb{E} o_{u,i}=1*p_{u,i} + 0*(1-p_{u,i}) = p_{u,i} \right) \\
&= \mathcal{L}
\end{aligned}
$$

ì™€ ê°™ì´ ì •ë¦¬ëœë‹¤.  
=> ë”°ë¼ì„œ ìž„ì˜ì˜ $\mathcal{D}$ì— ëŒ€í•˜ì—¬ $\mathcal{L}_{\text{IPS}}$ë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆë‹¤ë©´, í‰ê· ì„ í†µí•´ ideal lossì¸ $\mathcal{L}$ì„ êµ¬í•  ìˆ˜ ìžˆë‹¤.

### Propensity Estimation
í•œíŽ¸, $\mathcal{L}_{\text{IPS}}$ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ propensity score $p_{u,i}$ë¥¼ ì•Œì•„ë‚´ì•¼ í•˜ì§€ë§Œ, ëª¨ì§‘ë‹¨ì˜ ë¶„í¬ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì´ ë˜í•œ ì¶”ì •í•´ì•¼ë§Œ í•œë‹¤.

ì°¸ê³ ë¡œ, [PROPCARE](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a237f11d6aad94f59a182d70405d3fdb-Abstract-Conference.html)ì—ì„œëŠ” popularityì—ë§Œ ì˜ì¡´í•˜ëŠ” single-factor bias ê´€ì ì—ì„œ propensityë¥¼ ì¶”ì •í•˜ì˜€ë‹¤.  
ê¸°ì¡´ì˜ ë‹¤ë¥¸ ì—°êµ¬ ë˜í•œ positivity or popularity biasì—ë§Œ ì˜ì¡´í•œë‹¤ëŠ” ê´€ì ì—ì„œ propensityë¥¼ ì¶”ì •í•˜ì˜€ìœ¼ë‚˜, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘˜ ëª¨ë‘ë¥¼ ê³ ë ¤í•˜ëŠ” ***Milti-factorial bias***ê´€ì ì„ ì±„íƒí•œë‹¤.  

- positivity bias ê´€ì ì—ì„œ, $\hat{p}_{u,i}^{\text{pos}}=P\left(o=1 | y=y_{u,i}\right)$
- popularity bias ê´€ì ì—ì„œ, $\hat{p}_{u,i}^{\text{pop}}=P\left(o=1 | i \right)$ $\leftarrow$ item = i ì¼ ë•Œ  

ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ multi-factorial bias ê´€ì ì—ì„œ propensity scoreë¥¼ ì¶”ì •í•˜ë©´, ì•„ëž˜ì™€ ê°™ë‹¤.  
$$
\hat{p}_{u,i}^{\text{mul}}=P\left(o=1 | y=y_{u,i}, i \right)=\frac{P\left(y=y_{u,i}, i | o=1 \right)P\left(o=1\right)}{P\left(y=y_{u,i}, i \right)}
$$

ì´ ë•Œ data sparsity ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Laplace smoothing ë“± ê·¼ì‚¬ë¥¼ í†µí•˜ì—¬ $\hat{p}_{u,i}^{\text{mul}}$ë¥¼ êµ¬í•œë‹¤.

### Predicted rating score $\hat{y}_{u,i} Estimation$
$\hat{y}_{u,i}$ì˜ ê²½ìš° ë³¸ ë…¼ë¬¸ì—ì„œëŠ” standard Matrix Factorization (MF) modelì„ ì‚¬ìš©í•œë‹¤.
$$
\hat{y}_{u,i} = p_u ^T q_i + a_u + b_i + c, \quad \Theta = \{p_u, q_i, a_u, b_i, c\}
$$
ë‹¤ë¥¸ ë°©ë²•ì„ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ë‹¤.


### Total Loss function
ì´ì œ, ë§ˆì§€ë§‰ìœ¼ë¡œ Total Loss functionì„ êµ¬í•´ì•¼ í•œë‹¤.  
$\mathcal{L}_{\text{IPS}}$ì˜ ê¸°ëŒ“ê°’ì´ ideal loss $\mathcal{L}$ë¡œ ì¢‹ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì—, $\mathcal{L}_{\text{IPS}}$ë¥¼ ì›í˜•ìœ¼ë¡œ total lossë¥¼ êµ¬í•œë‹¤.  

ë¨¼ì €, $p, y$ì— ëŒ€í•´ ì¶”ì •ê°’ì„ ì‚¬ìš©í•œ IPS lossë¥¼ êµ¬í•˜ë©´, ($\hat{y}$ì˜ ê²½ìš° MFë¥¼ ì‚¬ìš©.)
$$
\mathcal{L}_{\text{MF-IPS}^{\text{Mul}}}
= \frac{1}{|\mathcal{D}|}\sum_{u,i \in \mathcal{D}} \frac{1}{\hat{p}_{u,i}^{\text{mul}}}\delta(\hat{y}_{u,i}, y_{u,i})
$$
ì´ë‹¤.

ì´ ë•Œ overfittingì„ ë§‰ê¸° ìœ„í•´ $L_2$-regularization termì„ ë„£ìœ¼ë©´, Total LossëŠ” ì•„ëž˜ì™€ ê°™ë‹¤.
$$
\begin{aligned}
\mathcal{L}_{\text{total}}
&= \mathcal{L}_{\text{MF-IPS}^{\text{Mul}}} \left(\Theta\right) \\
&= \frac{1}{|\mathcal{D}|}\sum_{u,i \in \mathcal{D}} \frac{1}{\hat{p}_{u,i}^{\text{mul}}}\delta(\hat{y}_{u,i}, y_{u,i}) + \lambda||\Theta||_2 ^2\\
\end{aligned}
$$

- ë…¼ë¬¸ì—ì„œëŠ” ì˜ˆì‹œë¡œ $\delta(\hat{y}_{u,i}, y_{u,i})=(\hat{y}_{u,i} - y_{u,i})^2$ ì™€ ê°™ì€ MSEë¡œ ì„¤ëª…í•˜ì˜€ì§€ë§Œ, ì´í›„ Table 1ì—ì„œ ë³¼ ìˆ˜ ìžˆë“¯ì´ ë‹¤ë¥¸ ì˜¤ì°¨ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.  

# Method (Solution)
- For updating our parameters $\theta \in \Theta$, we use ***alternating gradient descent*** algorithm and ***Adam*** optimizer.
- Eq. 19: $\quad \theta_{t} = \text{ADAM}\left(\theta_{t-1}, \nabla\theta_{t-1} \mathcal{L}_{\text{MF-IPS}^{\text{Mul}}}\right)$
  ![Algorithm1](figures/Algorithm-1.png)

# Experiments & Result
> RQ1: Does our proposed multifactorial method better mitigate the effect of bias in logged rating data than existing single-factor debiasing methods?

> (RQ2) How do varying smoothing parameters and our alternating gradient descent approach affect our multifactorial method?

> (RQ3) Can our multifactorial method $\text{MF-IPS}^{\text{Mul}}$ robustly mitigate the effect of selection bias in scenarios where the effect of two factors on bias is varied?

## Datasets
- Real world datasets which widely used to evaluate de-biasing methods.
  - [Yahoo!R3](https://dl.acm.org/doi/abs/10.1145/1639714.1639717)
  - [Coat](https://proceedings.mlr.press/v48/schnabel16.html?ref=https://githubhelp.com)
- training set: biased ratings $\rightarrow$ training / validation setìœ¼ë¡œ 4:1 ë¶„í• .
- test set: MCAR set (no selection bias == unbiased data $\mathcal{M}$)

## Error Function $\delta(\hat{y}_{u,i}, y_{u,i})$

- mean squared error (MSE)
- mean absolute error (MAE)
- root mean square error (RMSE)
- average RMSE performance per user ($\text{RMSE}_ð‘ˆ$)
- average RMSE performance per item ($\text{RMSE}_I$)
  - RMSE score for each individual user/item separately and then average them.

## Baselines (Methods) in Single-factor bias
- No IPS model, Ignore any selection bias
  - Avg model
  - MF model
  - VAE model
- IPS model, De-biased method
  - $\text{MF-IPS}^{\text{MF}}$: MF model - propensity estimation using MF with logistic regression
  - $\text{MF-IPS}^{\text{Pop}}$: MF model with $\hat{p}_{u,i}^{\text{pop}}$
  - $\text{MF-IPS}^{\text{Pos}}$: MF model with $\hat{p}_{u,i}^{\text{pos}}$

> Paper: Multi-factorial Bias $\text{MF-IPS}^{\text{Mul}}$: MF model with $\hat{p}_{u,i}^{\text{mul}}$

## Hyper-parameters tuning

### In the MF-based methods
- learning rate: $\eta$
- $L_2$ regularization weights: $\lambda$
- dimension of embeddings of users and items: $d$

### In the VAE method
- learning rate
- regularization weights
- dimension of the latent representation
- Kullback-Leibler term

### Paper method (multi-factorial bias)
- learning rate: $\eta$
- $L_2$ regularization weights: $\lambda$
- dimension of embeddings of users and items: $d$
- smoothing parameters: $\alpha_1, \alpha_2$

![Table1](figures/Table-1.png)

## RQ1 Answer: [Model Performance]
> RQ1: Does our proposed multifactorial method better mitigate the effect of bias in logged rating data than existing single-factor debiasing methods?  

- Yes. ëª¨ë“  Error Function $\delta$ì—ì„œ paperì˜ ëª¨í˜•ì´ ê°€ìž¥ Error evaluationì´ ë‚®ë‹¤. ë‘ dataset ëª¨ë‘
- $\text{MF-IPS}^{\text{Mul}} \succ \text{MF-IPS}^{\text{Pos}} \succ \text{MF-IPS}^{\text{Pop}}$: positivity bias has a stronger effect than popularity bias in rating predictions.

## RQ2 Answer: [Alternating GD & Smoothing]
> (RQ2) How do varying smoothing parameters and our alternating gradient descent approach affect our multifactorial method?

- ë‚˜ì¤‘ì— ìž‘ì„±. (ë…¼ë¬¸ 5.3)

## RQ3 Answer: [Robustness]
> (RQ3) Can our multifactorial method $\text{MF-IPS}^{\text{Mul}}$ robustly mitigate the effect of selection bias in scenarios where the effect of two factors on bias is varied?

- ë‚˜ì¤‘ì— ìž‘ì„±. (ë…¼ë¬¸ 6.1-6.2)

# Conclusion
> Multi-factorial Selection Bias $\text{MF-IPS}^{\text{Mul}}$: MF model with $\hat{p}_{u,i}^{\text{mul}}$ $\leftarrow$ Popularity bias & Positivity bias

1. Better Performance
2. More Stable from the data sparsity problem
3. More Robust

than SOTA (Single-factor Selection Bias model)
